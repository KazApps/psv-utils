# This script is based on https://github.com/tayayan/cshogi_util/blob/main/psvscore_to_dlvalue.py

import argparse
import math
import os
from typing import Tuple

from cshogi import Board, PackedSfenValue
from cshogi.dlshogi import make_input_features
import numpy as np
from numpy.typing import NDArray
import onnxruntime as ort
from tqdm import tqdm

import utils


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Replace the scores in a PSV file with evaluation values generated by dlshogi.")
    parser.add_argument("input_file", type=str, help="Input file (.bin)")
    parser.add_argument("output_file", type=str,
                        help="Output file (.bin). If left empty, the input file will be rescored and overwritten.")
    parser.add_argument("--score-scaling", type=float, default=600.0, help="Score scaling", dest="score_scaling")
    parser.add_argument("--blend-ratio", type=float, default=1.0,
                        help="Weight of the new score when blending with the original."
                        "1.0 = use only the new score.", dest="blend_ratio")
    parser.add_argument("--batch-size", type=int, default=16384, help="Batch size", dest="batch_size")
    parser.add_argument("--chunk-size", type=int, default=10**7, help="Chunk size", dest="chunk_size")
    parser.add_argument("--resume", action="store_true", help="Resume from the middle of the file", dest="resume")

    parser = utils.configure_session_args(parser)

    return parser.parse_args()


def verify_files(input_file: str, output_file: str, resume: bool) -> Tuple[int, int, bool]:
    in_place = not output_file

    if not os.path.exists(input_file):
        raise FileNotFoundError(f"Input file {input_file} does not exist.")

    if not input_file.endswith(".bin"):
        raise ValueError("Input file must be a PackedSfenValue file (.bin).")

    if not in_place and not output_file.endswith(".bin"):
        raise ValueError("Output file must be a PackedSfenValue file (.bin).")

    # Calculate the number of all positions from the size of the input file (each position is 40 bytes)
    file_size = os.path.getsize(input_file)
    total_positions = file_size // PackedSfenValue.itemsize
    processed_positions = 0

    if file_size % PackedSfenValue.itemsize != 0:
        raise ValueError(f"Input file {input_file} is broken (not a multiple of {PackedSfenValue.itemsize}).")

    if resume:
        if in_place:
            raise RuntimeError("Resumption is not supported in in-place mode.")

        if not os.path.exists(output_file):
            raise FileNotFoundError(f"Output file {output_file} does not exist.")

        offset = os.path.getsize(output_file)
        processed_positions = offset // PackedSfenValue.itemsize

        if offset % PackedSfenValue.itemsize != 0:
            raise ValueError(f"Output file {output_file} is broken (not a multiple of {PackedSfenValue.itemsize}).")

    return total_positions, processed_positions, in_place


def process_chunk(board: Board, chunk: NDArray[PackedSfenValue],
                  input_features1: NDArray[np.float32],
                  input_features2: NDArray[np.float32],
                  score_scaling: float,
                  batch_size: int,
                  session: ort.InferenceSession,
                  bar: tqdm) -> NDArray[np.int16]:
    read_size = len(chunk)
    win_rate = np.empty(read_size, dtype=np.float32)
    num_batches = math.ceil(read_size / batch_size)

    for b in range(num_batches):
        start = b * batch_size
        end = min(start + batch_size, read_size)
        current_batch = end - start

        # Feature creation per batch
        for j, idx in enumerate(range(start, end)):
            board.set_psfen(chunk[idx]["sfen"])
            make_input_features(board, input_features1[j], input_features2[j])

        # Inference
        win_rate[start:end] = utils.inference(input_features1[:current_batch], input_features2[:current_batch], session)[0]

        # Update progress bar
        bar.update(current_batch)

    # Convert win rate to score
    scores = utils.convert_to_score(win_rate, score_scaling)

    return scores


def rescore(input_path: str,
            output_path: str,
            num_positions: int,
            offset: int,
            score_scaling: float,
            blend_ratio: float,
            batch_size: int,
            chunk_size: int,
            resume: bool,
            session: ort.InferenceSession) -> None:
    """
    Replace the scores in a PSV file with evaluation values generated by dlshogi.
    """

    input_features1, input_features2 = utils.allocate_input_features(batch_size)
    processed_positions = offset

    # Board object
    board = Board()

    with tqdm(total=num_positions, initial=processed_positions) as bar:
        with open(output_path, "ab" if resume else "wb") as f_out:
            while processed_positions < num_positions:
                remaining_positions = num_positions - processed_positions
                read_size = min(remaining_positions, chunk_size)

                # Read a chunk from file
                psvs = np.fromfile(
                    input_path,
                    count=read_size,
                    offset=processed_positions * PackedSfenValue.itemsize,
                    dtype=PackedSfenValue
                )

                if len(psvs) != read_size:
                    raise ValueError(f"Failed to read {read_size} positions from the input file.")

                scores = process_chunk(board, psvs, input_features1, input_features2, score_scaling, batch_size, session, bar)
                psvs["score"] = scores * blend_ratio + psvs["score"] * (1 - blend_ratio)

                # Append to output file
                psvs.tofile(f_out)
                processed_positions += read_size


def rescore_inplace(input_path: str,
                    num_positions: int,
                    score_scaling: float,
                    blend_ratio: float,
                    batch_size: int,
                    chunk_size: int,
                    session: ort.InferenceSession) -> None:
    """
    Replace the scores in a PSV file with evaluation values generated by dlshogi, in-place.
    """

    input_features1, input_features2 = utils.allocate_input_features(batch_size)
    processed_positions = 0

    # Board object
    board = Board()

    psvs = np.memmap(input_path, mode="r+", dtype=PackedSfenValue)

    with tqdm(total=num_positions) as bar:
        while processed_positions < num_positions:
            remaining_positions = num_positions - processed_positions
            read_size = min(remaining_positions, chunk_size)

            # Read a chunk
            chunk = psvs[processed_positions:processed_positions+read_size]

            if len(chunk) != read_size:
                raise ValueError(f"Failed to read {read_size} positions from the input file.")

            scores = process_chunk(board, chunk.copy(), input_features1, input_features2, score_scaling, batch_size, session, bar)
            chunk["score"] = scores * blend_ratio + chunk["score"] * (1 - blend_ratio)
            psvs.flush()
            processed_positions += read_size

    psvs.flush()


def main() -> None:
    args = parse_args()
    total_positions, processed_positions, in_place = verify_files(args.input_file, args.output_file, args.resume)

    print("-----------------------------------------")
    print(f"Input file          : {args.input_file}")
    print(f"Number of positions : {total_positions}" + (f" (Resume from {processed_positions})" if args.resume else ""))
    print(f"Output file         : {args.input_file + ' (in-place)' if in_place else args.output_file}")
    print(f"Model path          : {args.model_path}")
    print(f"Score scaling       : {args.score_scaling}")
    print(f"Blend ratio         : {args.blend_ratio}")
    print(f"Batch size          : {args.batch_size}")
    print(f"Chunk size          : {args.chunk_size}")
    print(f"Device ID           : {args.device_id}")
    print(f"Enable CUDA         : {args.enable_cuda}")
    print(f"Enable TensorRT     : {args.enable_tensorrt}")
    print("-----------------------------------------", end="\n\n")

    # Creating an ONNX Runtime session
    session = utils.create_session(args)
    print("ONNX Runtime session is ready.", end="\n\n")

    if in_place:
        rescore_inplace(args.input_file,
                        total_positions,
                        args.score_scaling,
                        args.blend_ratio,
                        args.batch_size,
                        args.chunk_size,
                        session)
    else:
        rescore(args.input_file,
                args.output_file,
                total_positions,
                processed_positions,
                args.score_scaling,
                args.blend_ratio,
                args.batch_size,
                args.chunk_size,
                args.resume,
                session)

    print("The score replacement process is complete.")

    if not in_place:
        print(f"Output file : {args.output_file}")

if __name__ == "__main__":
    main()
